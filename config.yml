# Fine-tuning Configuration for Qwen Model

# Model Configuration
model:
  name: "Qwen/Qwen2-1.5B"
  dtype: "float32"
  low_cpu_mem_usage: true
  trust_remote_code: true

# LoRA Configuration
lora:
  r: 64
  lora_alpha: 16
  lora_dropout: 0.05
  target_modules:
    - "q_proj"
    - "v_proj"
  bias: "none"
  task_type: "CAUSAL_LM"

# Dataset Configuration
dataset:
  train_file: "train.jsonl"
  test_file: "test.jsonl"
  text_column: "text"
  max_length: 512
  padding: "max_length"
  truncation: true

# Training Arguments
training:
  output_dir: "./qwen_finetune"
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 4
  learning_rate: 0.0002
  num_train_epochs: 3
  warmup_steps: 100
  logging_steps: 10
  save_steps: 500
  save_total_limit: 3
  eval_steps: 250
  evaluation_strategy: "no"
  fp16: false
  use_cpu: true
  optim: "adamw_torch"
  weight_decay: 0.01
  max_grad_norm: 1.0
  lr_scheduler_type: "cosine"
  
# Evaluation Configuration
evaluation:
  metric: "accuracy"
  eval_on_start: false
  load_best_model_at_end: false
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# Generation Configuration
generation:
  max_new_tokens: 100
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  num_beams: 1
  
# Hardware Configuration
hardware:
  device: "cpu"
  num_workers: 4
  pin_memory: false
  
# Logging Configuration
logging:
  log_level: "info"
  log_to_file: true
  log_file: "training.log"
  report_to: []

# Reproducibility
seed: 42
deterministic: true

